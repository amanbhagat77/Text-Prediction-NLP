---
title: "Exploratory Analysis on Swift Key Capstone Project"
author: "Aman Bhagat"
date: "16/08/2020"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(NLP)
library(tm)
library(textmineR)
library(RWeka)
library(wordcloud)
library(dplyr)

```

## Overview

This is an exploratory analysis on the given dataset to understand the distribution of words and relationship between the words in the corpora. The goal of this analysis is to get breif summary and important features of the dataset which would be useful in model building process.



## DataSet

We have downloaded the dataset and established connection to the dataset.

```{r datset, echo=TRUE}
#Establishing connection to the English version of the twitter, Blogs and news dataset

con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")

```

```{r reading, cache = TRUE, warning = FALSE}
# Reading lines from each of the dataset
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
```

Summary on length of each data set lines can be observed below:
  
  ```{r summary, cache = TRUE}
summary(nchar(lines_twitter))
summary(nchar(lines_blogs))
summary(nchar(lines_news))

```

## Data Sampling

For this purpose we took 5% of data from each file because if we take data more thatn that then it cannot be processed as it will require more memory size which is not available in a general commputing system. Each of the file is then combined and proceeded with the data cleaning process. 

```{r sampling, cache = TRUE}
set.seed(1725)
twitter <- sample(lines_twitter, length(lines_twitter)*0.05)
blogs <- sample(lines_blogs, length(lines_blogs)*0.05)
news <- sample(lines_news, length(lines_news)*0.05)

corpus <- c(twitter, blogs, news)
corpus <- iconv(corpus, "UTF-8","ASCII", sub = "")
length(corpus)

#clearing Memory
rm(blogs,con_blogs,con_news,con_twitter,lines_blogs,lines_news,lines_twitter)
```

## Data Cleaning

Here we cleanded the data by taking the following measures:
1. Removed any extra white spacae
2. Converted all the words into lower case as to reduce ambiguity
3. Removed punctuation as it has no meaning
4. Removed Numbers
5. Converted the file into plain text document
6. Removed stop words as it has no meaning and present in bulk in the dataset

```{r datacleaning, cache = TRUE}
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
```

## Exploratory Analysis

Now we will create  unigram, bigram and trigram of the dataset, which will represent the frequency of each wset of words. To reduce the computation we have taken most frequent terms for representing purpose. In case of unigram we took the words which have more than 60 occurence. In case of Bigram we took the words which have more than 40 occurrence and finally for trigrams we took the words which have more than 10 occurence.

```{r n-gram, cache = TRUE}
memory.limit(size = 56000)
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 10)
```
```{r bigram,cache = TRUE}
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = bigram))
bigram_freqTerm <- findFreqTerms(bigram_tdm,lowfreq=40)
```
```{r trigram, cache = TRUE}
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = trigram))
trigram_freqTerm <- findFreqTerms(trigram_tdm,lowfreq=10)
```

```{r quadgram, cache = TRUE}
quadgram <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
quadgram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = quadgram))
quadgram_freqTerm <- findFreqTerms(quadgram_tdm,lowfreq=10)
```
### Distribution of each gram

Here we are showing the distribution of unigram, bigram and trigram to undestand the frequency of each words. we have also made the word cloud to clearly depict the words which are more used in the dataset.

```{r , echo=FALSE}
gc()
```

```{r Unigram,cache = TRUE, eval=TRUE}
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(token=names(unigram_freq[unigram_ord]), count=unigram_freq[unigram_ord])
rm(unigram, unigram_tdm,unigram_freqTerm,unigram_ord)
```

```{r unigram_plot, eval = TRUE}
ggplot(unigram_freq[1:25,], aes(factor(word, levels = unique(word)), frequency)) +
  geom_bar(stat = 'identity')+
  theme(axis.text.x=element_text(angle=90))+
  xlab('Unigram')+
  ylab('Frequency')
```


```{r , echo=FALSE}
gc()
```

```{r word_cloud_unigram, eval=TRUE}
wordcloud(unigram_freq$word, unigram_freq$frequency, max.words=40, colors=brewer.pal(8, "Set1"))
```

From the above distribution and word cloud we can see that "Just", "Like", "will", "one" and "can" are the top 5 most occurred word in the dataset. The Frequency is above and approx 20000 in the dataset.


```{r bigram_plot, cache = TRUE}
bigram_freq <- rowSums(as.matrix(bigram_tdm[bigram_freqTerm,]))
bigram_ord <- order(bigram_freq, decreasing = TRUE)
bigram_freq <- data.frame(token=names(bigram_freq[bigram_ord]), count=bigram_freq[bigram_ord])
rm(bigram, bigram_tdm,bigram_freqTerm,bigram_ord)

ggplot(bigram_freq[1:20,], aes(factor(word, levels = unique(word)), frequency)) +
  geom_bar(stat = 'identity')+
  theme(axis.text.x=element_text(angle=90))+
  xlab('Bigram')+
  ylab('Frequency')
```

```{r bigram_cloud}
wordcloud(bigram_freq$word, bigram_freq$frequency, max.words=30, colors=brewer.pal(8, "Set1"))
```

From the above distribution and word cloud we can see that "right now", "can't wait", "don't know", "last night" and "feel like" are the top 5 most occurred word in the dataset. The information will help us to predict the next word as required in the future modelling process.

```{r triigram, cache = TRUE, eval=TRUE}
trigram_freq <- rowSums(as.matrix(trigram_tdm[trigram_freqTerm,]))
trigram_ord <- order(trigram_freq, decreasing = TRUE)
trigram_freq <- data.frame(token=names(trigram_freq[trigram_ord]), count=trigram_freq[trigram_ord])
rm(trigram, trigram_tdm,trigram_freqTerm,trigram_ord)

ggplot(trigram_freq[1:15,], aes(factor(word, levels = unique(word)), frequency)) +
  geom_bar(stat = 'identity')+
  theme(axis.text.x=element_text(angle=90))+
  xlab('Trigram')+
  ylab('Frequency')

```


```{r trigram_cloud}
wordcloud(trigram_freq$word, trigram_freq$frequency, max.words=15, colors=brewer.pal(8, "Set1"))
```

Here are the top 15 most occuring trigrams in the dataset. This gives us the information that which combination of words are being used mostly and can be helpful in predicting the next word.

```{r quadgram}
quadgram_freq <- rowSums(as.matrix(quadgram_tdm[quadgram_freqTerm,]))
quadgram_ord <- order(quadgram_freq, decreasing = TRUE)
quadgram_freq <- data.frame(token=names(quadgram_freq[quadgram_ord]), count=quadgram_freq[quadgram_ord])
rm(quadgram, quadgram_tdm,quadgram_freqTerm,quadgram_ord)
```

## Proportion Analysis

In this analysis we will try to find out that how many words from sorted frequency dictionary can cover 50% of the dataset. For this we will calculate proportion of each word in the dataset and find out the answer to the above query.

```{r prop, cache=TRUE, eval=TRUE}
unigram_freq <- unigram_freq %>% mutate(prop = frequency/sum(frequency))
unigram_freq <- as.data.frame(unigram_freq)
head(unigram_freq)

n <- nrow(unigram_freq)
count <- 1
sum <- 0
while(count <= n){
  if(round(sum(unigram_freq$prop[1:count]),2) == 0.50 ){
    break
  }
  count <- count + 1
  
}
## Total Number of words
count
## Total Percentage of the word
total.percentage = (count/n)*100
total.percentage

```

From the above calculation we found that there are 360 (~8%) words which covers 50% of the proportion in the datset. Therefore this words we can keep in the sorted doictionary, which we can be referred for the prediction model.

## Summary

From the above analysis we found out the distribution of each word in the datset without any stop words. After cleaning the dataset, we reduced the dataset size and removed words which had no meanings like stop words and punctuation from the datset. We have also found the distribution of combination of words like bigrams and trigrams and from that we concluded that as the the combination of number of words increased, the total frequency has decreased. We have not removed any profanity words from the dataset as we did not encounter any in this sample of words which leads to the conclusion that the number of profanity words in the dataset is comparetively less. With the proportion analysis we got further insight to the data as we found that 50% of the words in the dataset is covered by only 8% percent of the words. Identifying these words would be very usefull as these words are used more often. These analysis would be helpful in the model building process.
