---
title: "Exploratory Analysis"
author: "Aman Bhagat"
date: "13/08/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)

```

## Overview
 
 
This is an exploratory analysis on the given dataset to understand the distribution of words and relationship between the words in the corpora. The goal of this analysis is to get breif summary and important features of the dataset which would be useful in model building process.

## DataSet

We have downloaded the dataset and established connection to the dataset.

```{r datset, echo=TRUE}
#Establishing connection to the English version of the twitter, Blogs and news dataset

con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")

```

```{r reading, cache = TRUE, warning = FALSE}
# Reading lines from each of the dataset
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
```

Summary on length of each data set lines can be observed below:

```{r summary, cache = TRUE}
summary(nchar(lines_twitter))
summary(nchar(lines_blogs))
summary(nchar(lines_news))

```

## Exploratory Analysis

### Twitter Dataset

Now we we will analyse the distribution pattern of the words in the distribution. At first we will observe the frequency distribution of each word in the twitter dataset.

```{r twitter, cache = TRUE, warning =  FALSE}
library(ngram)
library(dplyr)
library(ggplot2)
lines_twitter <- concatenate(lines_twitter)
lines_twitter <- preprocess(lines_twitter, case = "lower", remove.punct = TRUE)
words_twitter <- strsplit(lines_twitter, " ")
words_twitter <- data.frame(Words = unlist(words_twitter))
words_twitter$count <- 1
words_twitter <- words_twitter %>% group_by(Words) %>% summarize(count = sum(count))
words_twitter <- words_twitter[order(-words_twitter$count),]
n <- length(lines_twitter)
words_twitter <- words_twitter %>% mutate(prop = count/sum(count))
words_twitter <- as.data.frame(words_twitter)
head(words_twitter)
```

Above table indicates the proportion of each word in the twitter dataset.

```{r plot}
#Plot
g <- ggplot(data = words_twitter[1:15,], aes(x = Words, y = count))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity", fill  = "steelblue")
g <- g + labs(title = "Top 15 most frequent used words in Twitter Dataset", y = "Frequency", x = "Words")
g <- g + guides(fill = FALSE)
g
```

From the above graph plot we can infer top 15 most frequent words used in the twiter dataset, which indicates that the word **"THE"** has been used most and the proportion is 5% as compared to the other words.

```{r two-gram, cache = TRUE, warning = FALSE}
two_gram <- ngram(lines_twitter, sep = ", ", n = 2)
two_gram_twitter <- ( get.phrasetable ( two_gram ))

three_gram <- ngram(lines_twitter, sep = ", ", n = 3)
three_gram_twitter <- ( get.phrasetable ( three_gram ))

```

```{r plots}
library(gridExtra)
g <- ggplot(data = two_gram_twitter[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used two gram words in Twitter Dataset", y = "Frequency", x = "two-gram")
g <- g +  theme(axis.text.x=element_text(angle = 90))
g <- g + guides(fill = FALSE)
g1 <- g

g <- ggplot(data = three_gram_twitter[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used three gram words in Twitter Dataset", y = "Frequency", x = "three-gram")
g <- g +  theme(axis.text.x=element_text(angle = 90))
g <- g + guides(fill = FALSE)
g2 <- g

grid.arrange(g1,g2, ncol = 2)
```

The above two plots depict the distribution of two gram and three gram words in the twitter dataset.
Here we can see that "**of the**" and "**in the**" are used more that 1000 times as a combination in the dataset. "**One of the**" and "**a lot of**" have been used over 900 in the twitter dataset. This information gives boost to our prediction model.

### Blogs Dataset

Now we we will analyse the distribution pattern of the words in the distribution. At first we will observe the frequency distribution of each word in the Blogs dataset.

```{r Blogs, cache = TRUE}
lines_blogs <- concatenate(lines_blogs)
lines_blogs <- preprocess(lines_blogs, case = "lower", remove.punct = TRUE)
words_blogs <- strsplit(lines_blogs, " ")
words_blogs <- data.frame(Words = unlist(words_blogs))
words_blogs$count <- 1
words_blogs <- words_blogs %>% group_by(Words) %>% summarize(count = sum(count))
words_blogs <- words_blogs[order(-words_blogs$count),]
n <- length(lines_blogs)
words_blogs <- words_twitter %>% mutate(prop = count/sum(count))
words_blogs <- as.data.frame(words_blogs)
head(words_blogs)
```

Above table indicates the proportion of each word in the twitter dataset.

```{r plot2}
#Plot
g <- ggplot(data = words_blogs[1:15,], aes(x = Words, y = count))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity", fill  = "steelblue")
g <- g + labs(title = "Top 15 most frequent used words in Blogs Dataset", y = "Frequency", x = "Words")
g <- g + guides(fill = FALSE)
g
```

From the above graph plot we can infer top 15 most frequent words used in the twiter dataset, which indicates that the word **"THE"** has been used most and the proportion is 3% as compared to the other words.

```{r two-gram2, cache = TRUE, warning = FALSE, eval=FALSE}
two_gram <- ngram(lines_blogs, sep = ", ", n = 2)
two_gram_blogs <- ( get.phrasetable ( two_gram ))
```

```{r three-gram_blogs, cache=TRUE, warning = FALSE,eval=FALSE}
three_gram <- ngram(lines_blogs, sep = ", ", n = 3)
three_gram_blogs <- ( get.phrasetable ( three_gram ))

```

```{r plots2, eval = FALSE}
library(gridExtra)
g <- ggplot(data = two_gram_Blogs[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used two gram words in Blogs Dataset", y = "Frequency", x = "two-gram")
g <- g +  theme(axis.text.x=element_text(angle = 90))
g <- g + guides(fill = FALSE)
g1 <- g

g <- ggplot(data = three_gram_blogs[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used three gram words in Blogs Dataset", y = "Frequency", x = "three-gram")
g <- g +  theme(axis.text.x=element_text(angle = 90))
g <- g + guides(fill = FALSE)
g2 <- g

grid.arrange(g1,g2, ncol = 2)
```

The above two plots depict the distribution of two gram and three gram words in the twitter dataset.
Here we can see that "**of the**" and "**in the**" are used more that 1000 times as a combination in the dataset. "**One of the**" and "**a lot of**" have been used over 900 in the twitter dataset. This information gives boost to our prediction model.

### News Dataset

Now we we will analyse the distribution pattern of the words in the distribution. At first we will observe the frequency distribution of each word in the Blogs dataset.

```{r news, cache = TRUE}
lines_news <- concatenate(lines_news)
lines_news <- preprocess(lines_news, case = "lower", remove.punct = TRUE)
words_news <- strsplit(lines_news, " ")
words_news <- data.frame(Words = unlist(words_news))
words_news$count <- 1
words_news <- words_news %>% group_by(Words) %>% summarize(count = sum(count))
words_news <- words_news[order(-words_news$count),]
n <- length(lines_news)
words_news <- words_news %>% mutate(prop = count/sum(count))
words_news <- as.data.frame(words_news)
head(words_news)
```

Above table indicates the proportion of each word in the twitter dataset.

```{r plot3}
#Plot
g <- ggplot(data = words_news[1:15,], aes(x = Words, y = count))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity", fill  = "steelblue")
g <- g + labs(title = "Top 15 most frequent used words in News Dataset", y = "Frequency", x = "Words")
g <- g + guides(fill = FALSE)
g
```

From the above graph plot we can infer top 15 most frequent words used in the twiter dataset, which indicates that the word **"THE"** has been used most and the proportion is 5% as compared to the other words.

```{r two-gram3, cache = TRUE, warning = FALSE}
two_gram <- ngram(lines_news, sep = ", ", n = 2)
two_gram_news <- ( get.phrasetable ( two_gram ))
```

```{r three-gram_news, cache=TRUE, warning=FALSE}
three_gram <- ngram(lines_news, sep = ", ", n = 3)
three_gram_news <- ( get.phrasetable ( three_gram ))

```

```{r plots3}
library(gridExtra)
g <- ggplot(data = two_gram_news[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used two gram words in News Dataset", y = "Frequency", x = "two-gram")
g <- g +  theme(axis.text.x=element_text(angle = 90))
g <- g + guides(fill = FALSE)
g3 <- g

g <- ggplot(data = three_gram_news[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used three gram words in News Dataset", y = "Frequency", x = "three-gram")
g <- g +  theme(axis.text.x=element_text(angle = 90))
g <- g + guides(fill = FALSE)
g4 <- g

grid.arrange(g3,g4, ncol = 2)
```

The above two plots depict the distribution of two gram and three gram words in the News dataset.
Here we can see that "**of the**" and "**in the**" are used more that 1000 times as a combination in the dataset. "**One of the**" and "**a lot of**" have been used over 900 in the News dataset. This information gives boost to our prediction model. This is identical to what we have seen in the twitter dataset.

## Proprtion Analysis

In this analysis we want to see that how many unique words from each of the dataset will cover 50% of the total proportion

### Twitter Dataset

```{r prop_twitter, cache=TRUE}
n <- nrow(words_twitter)
count <- 1
sum <- 0
while(count <= n){
  if(round(sum(words_twitter$prop[1:count]),2) == 0.50){
    break
  }
  count <- count + 1
    
}
count
sum(words_twitter$prop)
total.percentage = (count/n)*100
total.percentage
```


