}                                                                               ## Unigram model
return(candidate_list)
}
## This Program will predict next words
## Function Name: find_predicted_words()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##
## Returns list of predicted words
find_predicted_words <- function(string){
candidate_list <- find_candidate_list(string, min_count = 2)
probability <- vector()
i <- 1
n <- length(candidate_list)
size <- length(unlist(strsplit(string , " ")))                   #size of the string
for (word in candidate_list){
probability[i] <- PKN(string, word, size + 1  )
i <- i + 1
}
words_with_probability <- data.frame(word = unlist(candidate_list), PKN = unlist(probability))
##print(words_with_probability)
words_with_probability <- words_with_probability[order(-words_with_probability$PKN)][1:3]
return(words_with_probability$word)
}
find_predicted_words(string = "thanks for the")
## This Program will predict next words
## Function Name: find_predicted_words()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##
## Returns list of predicted words
find_predicted_words <- function(string){
candidate_list <- find_candidate_list(string, min_count = 2)
probability <- vector()
i <- 1
n <- length(candidate_list)
size <- length(unlist(strsplit(string , " ")))                   #size of the string
for (word in candidate_list){
probability[i] <- PKN(string, word, size + 1  )
i <- i + 1
}
words_with_probability <- data.frame(word = unlist(candidate_list), PKN = unlist(probability))
##print(words_with_probability)
words_with_probability <- words_with_probability[order(-words_with_probability$PKN),][1:3]
return(words_with_probability$word)
}
find_predicted_words(string = "thanks for the")
## This Program will predict next words
## Function Name: find_predicted_words()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##
## Returns list of predicted words
find_predicted_words <- function(string){
candidate_list <- find_candidate_list(string, min_count = 2)
probability <- vector()
i <- 1
n <- length(candidate_list)
size <- length(unlist(strsplit(string , " ")))                   #size of the string
for (word in candidate_list){
probability[i] <- PKN(string, word, size + 1  )
i <- i + 1
}
words_with_probability <- data.frame(word = unlist(candidate_list), PKN = unlist(probability))
##print(words_with_probability)
words_with_probability <- words_with_probability$word[order(-words_with_probability$PKN)][1:3]
return(words_with_probability$word)
}
find_predicted_words(string = "thanks for the")
## This Program will predict next words
## Function Name: find_predicted_words()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##
## Returns list of predicted words
find_predicted_words <- function(string){
candidate_list <- find_candidate_list(string, min_count = 2)
probability <- vector()
i <- 1
n <- length(candidate_list)
size <- length(unlist(strsplit(string , " ")))                   #size of the string
for (word in candidate_list){
probability[i] <- PKN(string, word, size + 1  )
i <- i + 1
}
words_with_probability <- data.frame(word = unlist(candidate_list), PKN = unlist(probability))
##print(words_with_probability)
words_with_probability <- words_with_probability$word[order(-words_with_probability$PKN)][1:3]
return(words_with_probability)
}
find_predicted_words(string = "thanks for the")
## The candidate list is prepared based on the bigram model
##
## Function Name: find_candidate_list()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##      minimum Count : Minimum count of the found word
## Returns list of candidate words
find_candidate_list <- function(string, min_count){
wordslist <- unlist(strsplit(string, " "))
n <- length(wordslist)
word <- wordslist[n] ## word to be searched in the bigram model to get the list words which are followed by this word
candidate_list <- bigram_freq[grepl(paste("^",word,"$", sep = ""),bigram_freq$token1 ) & bigram_freq$count > min_count,]
candidate_list <- candidate_list$token2[order(-candidate_list$count)][1:25]
candidate_list <- candidate_list[!is.na(candidate_list)]                        ## Remove NA from the list
candidate_list <- candidate_list[candidate_list!= "s" & candidate_list!= "rt" & candidate_list!= "th" & candidate_list != "st"]
if(length(candidate_list) == 0){
candidate_list <- unigram_freq$token[order(-unigram_freq$count)][1:25]        ## If there is no match then return Top 50 words from
}                                                                               ## Unigram model
return(candidate_list)
}
## Step 2: Generate candidateList ####
# candidateList: list of seen word2s in a bigram model [word1,word2] with count >= min,
#               where word1 is the ith word in a string of length i.
candidateList <- function(string, dictlist, min = 2) {
#Inputs:
# string: a vector of split words
# dictlist: a list of ngram models
# min: only bigrams with a minimum frequency of "min" will be returned
#Output:
# result: a vector of split words that are candidate "next words" to a string, based on bigrams
if(length(grep(" ", string)) > 0){
string <- strsplit(string, split = " ")[[1]]
}
result <- dictlist[[2]][token1 == tail(string, 1) & count >= min][order(-count)][1:100]$token2
result <- result[result!= "s" & result!= "rt" & result!= "th" & result != "st"]
return(result)
}
find_predicted_words(string = "thanks for the")
find_predicted_words(string = "what are you")
find_predicted_words(string = "a case of")
find_predicted_words(string = "a case of")
## The candidate list is prepared based on the bigram model
##
## Function Name: find_candidate_list()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##      minimum Count : Minimum count of the found word
## Returns list of candidate words
find_candidate_list <- function(string, min_count){
wordslist <- unlist(strsplit(string, " "))
n <- length(wordslist)
word <- wordslist[n] ## word to be searched in the bigram model to get the list words which are followed by this word
candidate_list <- bigram_freq[grepl(paste("^",word,"$", sep = ""),bigram_freq$token1 ) & bigram_freq$count > min_count,]
candidate_list <- candidate_list$token2[order(-candidate_list$count)][1:100]
candidate_list <- candidate_list[!is.na(candidate_list)]                        ## Remove NA from the list
candidate_list <- candidate_list[candidate_list!= "s" & candidate_list!= "rt" & candidate_list!= "th" & candidate_list != "st"]
if(length(candidate_list) == 0){
candidate_list <- unigram_freq$token[order(-unigram_freq$count)][1:100]        ## If there is no match then return Top 50 words from
}                                                                               ## Unigram model
return(candidate_list)
}
## This Program will predict next words
## Function Name: find_predicted_words()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##
## Returns list of predicted words
find_predicted_words <- function(string){
candidate_list <- find_candidate_list(string, min_count = 2)
probability <- vector()
i <- 1
n <- length(candidate_list)
size <- length(unlist(strsplit(string , " ")))                   #size of the string
for (word in candidate_list){
probability[i] <- PKN(string, word, size + 1  )
i <- i + 1
}
words_with_probability <- data.frame(word = unlist(candidate_list), PKN = unlist(probability))
##print(words_with_probability)
words_with_probability <- words_with_probability$word[order(-words_with_probability$PKN)][1:5]
return(words_with_probability)
}
find_predicted_words(string = "a case of")
## This Program will predict next words
## Function Name: find_predicted_words()
## Called by: predict_next_word()
## Arguments:
##      input string  : String input by user
##
## Returns list of predicted words
find_predicted_words <- function(string){
candidate_list <- find_candidate_list(string, min_count = 2)
probability <- vector()
i <- 1
n <- length(candidate_list)
size <- length(unlist(strsplit(string , " ")))                   #size of the string
for (word in candidate_list){
probability[i] <- PKN(string, word, size + 1  )
i <- i + 1
}
words_with_probability <- data.frame(word = unlist(candidate_list), PKN = unlist(probability))
##print(words_with_probability)
words_with_probability <- words_with_probability$word[order(-words_with_probability$PKN)][1:5]
return(unlist(words_with_probability))
}
find_predicted_words(string = "a case of")
find_predicted_words(string = "lets go")
find_predicted_words(string = "are you kidding")
find_predicted_words(string = "live and I'd")
find_predicted_words(string = "me about his")
find_predicted_words(string = "arctic monkeys this")
find_predicted_words(string = "helps reduce your")
find_predicted_words(string = " to take a")
find_predicted_words(string = "to take a")
find_predicted_words(string = "to settle the")
find_predicted_words(string = "groceries in each")
find_predicted_words(string = "bottom to the")
save.image("F:/R Workspace/Capstone/New_Cleaned_Env/new_corpus.RData")
con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
twitter <- sample(lines_twitter, length(lines_twitter)*0.15)
blogs <- sample(lines_blogs, length(lines_blogs)*0.15)
news <- sample(lines_news, length(lines_news)*0.15)
corpus <- c(twitter, blogs, news)
corpus <- iconv(corpus, "UTF-8","ASCII", sub = "")
length(corpus)
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
library(tm)
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
inspect(corpus)
corpus[1:3]
length(corpus)
data <- corrpus[1:5]
data <- corpus[1:5]
data
inspect(data)
corpus-1 <- corpus[1:213483]
corpus_1 <- corpus[1:213483]
corpus_1 <- corpus[1:160112]
install.packages("RevoScaleR")
install.packages("RevoScaleR")
View(corpus)
getwd()
saveRDS(corpus)
?saveRDS
saveRDS(corpus, file = "corpus.RDS")
Corpus <-  readRDS("corpus.rds")
floor(26.4)
u <- data.frame()
x1 <- c(7, 1)                        # Column 1 of data frame 2
x2 <- c(4, 1)                        # Column 2 of data frame 2
x3 <- c(4, 3)
data_2 <- data.frame(x1, x2, x3)
rbind(u,data_2)
range(4)
range(1,4)
range(1:4)
?range
seq(1,4)
total_size <- length(Corpus)
total_split <- 4
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
for i in seq(1,total_split){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
rm(unigram,unigram_tdm)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
}
total_size <- length(Corpus)
total_split <- 4
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
rm(unigram,unigram_tdm)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
}
library(ggplot2)
library(NLP)
library(tm)
library(textmineR)
library(RWeka)
library(wordcloud)
total_size <- length(Corpus)
total_split <- 4
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
rm(unigram,unigram_tdm)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
}
total_size <- length(Corpus)
total_split <- 4
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
}
system.time(for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
})
total_size <- length(Corpus)
total_split <- 6
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
system.time(for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
})
corpus <- readRDS("corpus.RDS")
total_size <- length(Corpus)
total_split <- 6
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
system.time(for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
})
chunk_size <- floor(total_size/total_split)
split_index <- chunk_size
View(unigram_df)
View(unigram_df)
corpus <- readRDS("corpus.RDS")
total_size <- length(Corpus)
total_split <- 6
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
system.time(for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
})
corpus <- readRDS("corpus.RDS")
total_size <- length(Corpus)
total_size
total_size <- length(corpus)
corpus <- readRDS("corpus.RDS")
total_size <- length(corpus)
total_split <- 6
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
system.time(for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
g
})
memory.limit(56000)
corpus <- readRDS("corpus.RDS")
total_size <- length(corpus)
total_split <- 6
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
system.time(for (i in seq(1,total_split)){
temp_corpus  <- Corpus[start_index:split_index]
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
gc()
})
corpus <- readRDS("corpus.RDS")
total_size <- length(corpus)
total_split <- 6
chunk_size <- floor(total_size/total_split)
start_index <- 1
split_index <- chunk_size
unigram_df <- data.frame()
system.time(for (i in seq(1,total_split)){
temp_corpus  <- corpus[start_index:split_index]
c
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(temp_corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
rm(unigram_ord,unigram_tdm,unigram)
start_index <- chunk_size + 1
split_index <- split_index + chunk_size
unigram_df <- rbind(unigram_df,unigram_freq)
gc()
})
gc()
memory.size(max=F)
gc()
memory.size(max=F)
shiny::runApp('GitHub/Text-Prediction-NLP')
runApp('GitHub/Text-Prediction-NLP')
directory <- "C:/Users/amanb/Documents/GitHub/Text-Prediction-NLP"
setwd(directory)
runApp()
