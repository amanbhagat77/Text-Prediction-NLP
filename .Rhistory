g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent two gram words", y = "Frequency", x = "two-gram")
g
g <- ggplot(data = two_gram_twitter[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent two gram words", y = "Frequency", x = "two-gram")
g <- g +  theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
g
g <- ggplot(data = two_gram_twitter[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent two gram words in Twitter Dataset", y = "Frequency", x = "two-gram")
g <- g +  theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
g
g <- ggplot(data = two_gram_twitter[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used two gram words in Twitter Dataset", y = "Frequency", x = "two-gram")
g <- g +  theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
g
save.image("D:/Games/R Workspace/Capstone/SwitKey/.RData")
g <- ggplot(data = three_gram_twitter[1:15,], aes(x = ngrams, y = freq, fill = ngrams ))
g <- g + geom_bar(aes(reorder(ngrams, freq),freq), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used three gram words in Twitter Dataset", y = "Frequency", x = "two-gram")
g <- g +  theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
g
View(words_twitter)
View(words_twitter)
g <- ggplot(data = words_twitter[1:15,], aes(x = Words, y = count, fill = words ))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used three gram words in Twitter Dataset", y = "Frequency", x = "Words")
g
g <- ggplot(data = words_twitter[1:15,], aes(x = Words, y = count))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used three gram words in Twitter Dataset", y = "Frequency", x = "Words")
g
g <- ggplot(data = words_twitter[1:15,], aes(x = Words, y = count, fill  = "Blue"))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used words in Twitter Dataset", y = "Frequency", x = "Words")
g
g <- ggplot(data = words_twitter[1:15,], aes(x = Words, y = count, fill  = "Blue"))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used words in Twitter Dataset", y = "Frequency", x = "Words")
g <- g + guides(fill = FALSE)
g
g <- ggplot(data = words_twitter[1:15,], aes(x = Words, y = count, fill  = "steelblue"))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity")
g <- g + labs(title = "Top 15 most frequent used words in Twitter Dataset", y = "Frequency", x = "Words")
g <- g + guides(fill = FALSE)
g
g <- ggplot(data = words_twitter[1:15,], aes(x = Words, y = count))
g <- g + geom_bar(aes(reorder(Words, count),count), stat = "identity", fill  = "steelblue")
g <- g + labs(title = "Top 15 most frequent used words in Twitter Dataset", y = "Frequency", x = "Words")
g <- g + guides(fill = FALSE)
g
summary(lines)
summary(nchar(lines))
con <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
lines_news <- readLines(con)
summary(nchar(lines_news))
con <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
lines_news <- readLines(con)
con <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
lines_news <- readLines(con, warn = FALSE)
summary(nchar(lines_news))
con <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
n <- as.numeric(length(readLines(con)))
maxLength <- 0
while (i <= n) {
words <- strsplit(readLines(con)," ")[[1]]
currentLength <- length(words)
if(currentLength > maxLength){
maxLength <- currentLength
}
i <- i + 1
}
maxLength
i = 1
con <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
n <- as.numeric(length(readLines(con)))
maxLength <- 0
while (i <= n) {
words <- strsplit(readLines(con)," ")[[1]]
currentLength <- length(words)
if(currentLength > maxLength){
maxLength <- currentLength
}
i <- i + 1
}
n <- as.numeric(length(readLines(con)))
maxLength <- 0
while (i <= n) {
words <- strsplit(readLines(con)," ")[[1]]
currentLength <- length(words)
if(currentLength > maxLength){
maxLength <- currentLength
}
i <- i + 1
}
maxLength
i = 1
con <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
n <- as.numeric(length(readLines(con)))
maxLength <- 0
while (i <= n) {
words <- strsplit(readLines(con)," ")[[1]]
currentLength <- length(words)
if(currentLength > maxLength){
maxLength <- currentLength
}
i <- i + 1
}
n
length(lines)
save.image("D:/Games/R Workspace/Capstone/SwitKey/.RData")
colSums(words_twitter$count)
?colSums
colSums(words_twitter)
sum(words_twitter$count)
The above two plots depict the distribution of two gram and three gram words in the twitter dataset.
Here we can see that "**of the**" and "**in the**" are used more that 1000 times as a combination in the dataset. "**One of the**" and "**a lot of**" have been used over 900 in the twitter dataset. This information gives boost to our prediction model.
The above two plots depict the distribution of two gram and three gram words in the twitter dataset.
Here we can see that "**of the**" and "**in the**" are used more that 1000 times as a combination in the dataset. "**One of the**" and "**a lot of**" have been used over 900 in the twitter dataset. This information gives boost to our prediction model.
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
if(sum < 0.50){
sum <- sum + numeric(words_twitter$prop[count])
count <- count + 1
}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
lines_news <- readLines(con_news, encoding="UTF-8")
summary(nchar(lines_news))
rm(three_gram, three_gram_twitter)
rm(three_gram_news, two_gram,two_gram_news,w,w_news,words_new,words_twitter)
rm(words_news,two_gram_twitter)
rm(lines)
rm(lines_con)
save.image("D:/Games/R Workspace/Capstone/SwitKey/.RData")
#Establishing connection to the English version of the twitter, Blogs and news dataset
con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")
# Reading lines from each of the dataset
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
twitter <- sample(lines_twitter, length(lines_twitter)*0.5)
blogs <- sample(lines_blogs, length(lines_blogs)*0.5)
news <- sample(lines_news, length(lines_news)*0.5)
corpus <- c(twitter, blogs, news)
corpus <- iconv(corpus, "UTF-8","ASCII", sub = "")
length(corpus)
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
lines_news <- readLines("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", encoding="UTF-8")
#Establishing connection to the English version of the twitter, Blogs and news dataset
con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")
# Reading lines from each of the dataset
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
twitter <- sample(lines_twitter, length(lines_twitter)*0.5)
blogs <- sample(lines_blogs, length(lines_blogs)*0.5)
news <- sample(lines_news, length(lines_news)*0.5)
corpus <- c(twitter, blogs, news)
corpus <- iconv(corpus, "UTF-8","ASCII", sub = "")
length(corpus)
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
library(NLP)
library(ggplot2)
library(NLP)
library(tm)
library(textmineR)
#Establishing connection to the English version of the twitter, Blogs and news dataset
con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")
# Reading lines from each of the dataset
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
twitter <- sample(lines_twitter, length(lines_twitter)*0.5)
blogs <- sample(lines_blogs, length(lines_blogs)*0.5)
news <- sample(lines_news, length(lines_news)*0.5)
corpus <- c(twitter, blogs, news)
corpus <- iconv(corpus, "UTF-8","ASCII", sub = "")
length(corpus)
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- c(twitter, blogs, news)
#Establishing connection to the English version of the twitter, Blogs and news dataset
con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")
# Reading lines from each of the dataset
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
twitter <- sample(lines_twitter, length(lines_twitter)*0.05)
blogs <- sample(lines_blogs, length(lines_blogs)*0.05)
news <- sample(lines_news, length(lines_news)*0.05)
corpus <- c(twitter, blogs, news)
corpus <- iconv(corpus, "UTF-8","ASCII", sub = "")
length(corpus)
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
library(textmineR)
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
library(RWeka)
install.packages("RWeka")
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))
library(RWeka)
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
memory.size()
memory.limit()
install.packages("Matrix")
install.packages("Matrix")
gc
gc()
memory.limit(size = 56000)
memory.limit()
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
ggplot(unigram_freq[1:25,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Unigram')+
ylab('Frequency')
gc()
save.image("D:/Games/R Workspace/Capstone/SwitKey/.RData")
library(textmineR)
memory.limit(56000)
memory.limit()
memory.limit(600000)
memory.size()
gc()
memory.limit()
memory.limit(56000)
memory.size()
memory.limit()
gc()
memory.limit(size = 56000)
---
title: "Untitled"
author: "Aman Bhagat"
date: "16/08/2020"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(NLP)
library(tm)
library(textmineR)
library(RWeka)
```
## Overview
This is an exploratory analysis on the given dataset to understand the distribution of words and relationship between the words in the corpora. The goal of this analysis is to get breif summary and important features of the dataset which would be useful in model building process.
## DataSet
We have downloaded the dataset and established connection to the dataset.
```{r datset, echo=TRUE}
#Establishing connection to the English version of the twitter, Blogs and news dataset
con_twitter <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", "r")
con_news <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.news.txt", "r")
con_blogs <- file("D:/Games/R Workspace/Capstone/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", "r")
```
```{r reading, cache = TRUE, warning = FALSE}
# Reading lines from each of the dataset
lines_twitter <- readLines(con_twitter, warn=FALSE, encoding="UTF-8")
lines_news <- readLines(con_news, encoding="UTF-8")
lines_blogs <- readLines(con_blogs, warn=FALSE, encoding="UTF-8")
```
Summary on length of each data set lines can be observed below:
```{r summary, cache = TRUE}
summary(nchar(lines_twitter))
summary(nchar(lines_blogs))
summary(nchar(lines_news))
```
## Data Sampling
For this purpose I took 10% of each file. Then I combined the sampled data and proceeded with the data cleaning process.
```{r sampling, cache = TRUE}
twitter <- sample(lines_twitter, length(lines_twitter)*0.05)
blogs <- sample(lines_blogs, length(lines_blogs)*0.05)
news <- sample(lines_news, length(lines_news)*0.05)
corpus <- c(twitter, blogs, news)
corpus <- iconv(corpus, "UTF-8","ASCII", sub = "")
length(corpus)
```
## Data Cleaning
In this step I used Text Mining Package to clean up the data.
```{r datacleaning, cache = TRUE}
corpus <- VCorpus(VectorSource(corpus))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, PlainTextDocument)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
```
## Exploratory Analysis
Now we will create  n-grams of the dataset, which will represent the frequency of each gram
```{r n-gram, cache = TRUE}
unigram <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
unigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = unigram))
unigram_freqTerm <- findFreqTerms(unigram_tdm,lowfreq = 40)
```
```{r bigram,cache = TRUE}
gc()
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
bigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = bigram))
bigram_freqTerm <- findFreqTerms(bigram_tdm,lowfreq=40)
```
```{r trigram, cache = TRUE}
gc()
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
trigram_tdm <- TermDocumentMatrix(corpus, control = list(tokenize = trigram))
trigram_freqTerm <- findFreqTerms(trigram_tdm,lowfreq=10)
```
### Distribution of each gram
```{r unigram}
unigram_freq <- rowSums(as.matrix(unigram_tdm[unigram_freqTerm,]))
unigram_ord <- order(unigram_freq, decreasing = TRUE)
unigram_freq <- data.frame(word=names(unigram_freq[unigram_ord]), frequency=unigram_freq[unigram_ord])
ggplot(unigram_freq[1:25,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Unigram')+
ylab('Frequency')
```
ggplot(unigram_freq[1:25,], aes(factor(word, levels = unique(word)), frequency)) +
+   geom_bar(stat = 'identity')+
+   theme(axis.text.x=element_text(angle=90))+
+   xlab('Unigram')+
+   ylab('Frequency')
ggplot(unigram_freq[1:25,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Unigram')+
ylab('Frequency')
save.image("D:/Games/R Workspace/Capstone/SwitKey/.RData")
memory.limit(size = 56000)
gc()
ggplot(unigram_freq[1:25,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Unigram')+
ylab('Frequency')
library(ggplot2)
ggplot(unigram_freq[1:25,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Unigram')+
ylab('Frequency')
bigram_freq <- rowSums(as.matrix(bigram_tdm[bigram_freqTerm,]))
bigram_ord <- order(bigram_freq, decreasing = TRUE)
bigram_freq <- data.frame(word=names(bigram_freq[bigram_ord]), frequency=bigram_freq[bigram_ord])
ggplot(bigram_freq[1:20,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Bigram')+
ylab('Frequency')
library(ggplot2)
library(NLP)
library(tm)
library(textmineR)
library(RWeka)
bigram_freq <- rowSums(as.matrix(bigram_tdm[bigram_freqTerm,]))
bigram_ord <- order(bigram_freq, decreasing = TRUE)
bigram_freq <- data.frame(word=names(bigram_freq[bigram_ord]), frequency=bigram_freq[bigram_ord])
ggplot(bigram_freq[1:20,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Bigram')+
ylab('Frequency')
trigram_freq <- rowSums(as.matrix(trigram_tdm[trigram_freqTerm,]))
trigram_ord <- order(trigram_freq, decreasing = TRUE)
trigram_freq <- data.frame(word=names(trigram_freq[trigram_ord]), frequency=trigram_freq[trigram_ord])
ggplot(trigram_freq[1:15,], aes(factor(word, levels = unique(word)), frequency)) +
geom_bar(stat = 'identity')+
theme(axis.text.x=element_text(angle=90))+
xlab('Trigram')+
ylab('Frequency')
save.image("D:/Games/R Workspace/Capstone/SwitKey/.RData")
install.packages("wordcloud")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(NLP)
library(tm)
library(textmineR)
library(RWeka)
count <- count + 1
memory.limit(size = 56000)
memory.limit()
memory.limit(56000)
memory.limit(size = 56000)
gc()
memory.limit()
memory.size()
memory.size(max = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(NLP)
library(tm)
library(textmineR)
library(RWeka)
library(wordcloud)
unigram_freq <- unigram_freq %>% mutate(prop = frequency/sum(frequency))
library(dplyr)
unigram_freq <- unigram_freq %>% mutate(prop = frequency/sum(frequency))
unigram_freq <- as.data.frame(unigram_freq)
head(unigram_freq)
n <- nrow(unigram_freq)
count <- 1
sum <- 0
while(count <= n){
if(round(sum(unigram_freq$prop[1:count]),2) == 0.50){
break
}
count <- count + 1
}
count
sum(unigram_freq$prop)
total.percentage = (count/n)*100
total.percentage
n <- nrow(unigram_freq)
count <- 1
sum <- 0
c <- vector()
while(count <= n){
c[count] <- round(sum(unigram_freq$prop[1:count])
count <- count + 1
}
n <- nrow(unigram_freq)
count <- 1
sum <- 0
c <- vector()
while(count <= n){
c[count] <- round(sum(unigram_freq$prop[1:count]))
count <- count + 1
}
c
?list
n <- nrow(unigram_freq)
count <- 1
sum <- 0
c <- list()
while(count <= n){
c[count] <- round(sum(unigram_freq$prop[1:count]))
count <- count + 1
}
n <- nrow(unigram_freq)
count <- 1
sum <- 0
c <- list()
while(count <= n){
c[count] <- round(sum(unigram_freq$prop[1:count]), 4)
count <- count + 1
}
View(c)
View(c)
total.percentage = (423/n)*100
total.percentage
View(bigram_freq)
View(bigram_freq)
